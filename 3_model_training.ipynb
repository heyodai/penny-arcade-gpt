{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll take the processed data and train a language model. We want to try the following models:\n",
    "\n",
    "- GPT-2\n",
    "- BERT\n",
    "- RoBERTa\n",
    "- GPT-Neo\n",
    "- GPT-J\n",
    "- XLNet\n",
    "- T5\n",
    "\n",
    "The training steps are as follows:\n",
    "\n",
    "1. Load the data\n",
    "2. Tokenize the data\n",
    "3. Create a PyTorch dataset\n",
    "4. Create a PyTorch dataloader\n",
    "5. Create a PyTorch model\n",
    "6. Create a PyTorch optimizer\n",
    "7. Create a PyTorch scheduler\n",
    "8. Train the model\n",
    "9. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exhilarating,</td>\n",
       "      <td>We only know two people who don't live within ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Penny Arcade is A Comedy Bistro,</td>\n",
       "      <td>Over at Gabe's yesterday, I mentioned that Gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minus The Pope And A Rabbi,</td>\n",
       "      <td>I thought that I had uploaded the comic, but I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jealousy,</td>\n",
       "      <td>It's fine and everything, but there's all thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Forgot,</td>\n",
       "      <td>My man Pork mentioned this Star Wars fan-film ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                     Exhilarating,    \n",
       "1  Penny Arcade is A Comedy Bistro,    \n",
       "2       Minus The Pope And A Rabbi,    \n",
       "3                         Jealousy,    \n",
       "4                         I Forgot,    \n",
       "\n",
       "                                                text  \n",
       "0  We only know two people who don't live within ...  \n",
       "1  Over at Gabe's yesterday, I mentioned that Gui...  \n",
       "2  I thought that I had uploaded the comic, but I...  \n",
       "3  It's fine and everything, but there's all thes...  \n",
       "4  My man Pork mentioned this Star Wars fan-film ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('data/penny_arcade_processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/odai/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/odai/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/odai/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/odai/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/odai/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, pad_to_max_length=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the data\n",
    "\n",
    "# padded = pad_sequences(tokenized, maxlen=max_length, dtype=\"long\", \n",
    "#                         value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the attention mask\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(padded, df['title'].values, random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_mask, padded, random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# convert the data to tensors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# train_inputs = torch.tensor(train_inputs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# train_masks = torch.tensor(train_masks)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# validation_masks = torch.tensor(validation_masks)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_inputs\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39mint64))\n\u001b[1;32m     11\u001b[0m validation_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(validation_inputs\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64))\n\u001b[1;32m     12\u001b[0m train_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_labels\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# convert the data to tensors\n",
    "\n",
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# validation_inputs = torch.tensor(validation_inputs)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# validation_labels = torch.tensor(validation_labels)\n",
    "# train_masks = torch.tensor(train_masks)\n",
    "# validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs.astype(np.int64))\n",
    "validation_inputs = torch.tensor(validation_inputs.astype(np.int64))\n",
    "train_labels = torch.tensor(train_labels.astype(np.int64))\n",
    "validation_labels = torch.tensor(validation_labels.astype(np.int64))\n",
    "train_masks = torch.tensor(train_masks.astype(np.int64))\n",
    "validation_masks = torch.tensor(validation_masks.astype(np.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create the dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# freeze the model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# move the model to the GPU\n",
    "model.cuda()\n",
    "\n",
    "# define the optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# define the loss function\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# define the training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_accuracy += flat_accuracy(logits, b_labels)\n",
    "    print(f'Epoch {epoch}')\n",
    "    print(f'Training loss: {total_loss/len(train_dataloader)}')\n",
    "    print(f'Training accuracy: {total_accuracy/len(train_dataloader)}')\n",
    "\n",
    "# define the validation loop\n",
    "def evaluate(epoch):\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += flat_accuracy(logits, b_labels)\n",
    "    print(f'Validation loss: {total_loss/len(validation_dataloader)}')\n",
    "    print(f'Validation accuracy: {total_accuracy/len(validation_dataloader)}')\n",
    "    print(\"\")\n",
    "\n",
    "# define the accuracy function\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# train the model\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "epochs = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    train(_)\n",
    "    evaluate(_)\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained('models/distilbert-base-uncased')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained('models/distilbert-base-uncased')\n",
    "\n",
    "# # Path: 4_model_evaluation.ipynb\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # load the data\n",
    "# df = pd.read_csv('data/penny_arcade_processed.csv')\n",
    "# df.head()\n",
    "\n",
    "# # tokenize the data\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
